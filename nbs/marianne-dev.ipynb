{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prep data for the LSTM:\n",
    "1. all data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## using the neuralhydrology package\n",
    "## 1: prep the data according to the nh standard\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from fos import util\n",
    "from fos.data import snotelmeta \n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "from fos.util import get_wrf_data_points\n",
    "\n",
    "## \n",
    "def wrfread(datadir, gcm, exp, variant, domain, var):\n",
    "    modeldir = datadir + gcm + '/postprocess/'+domain + '/'\n",
    "    all_files = sorted(os.listdir(modeldir))\n",
    "    read_files = []\n",
    "    for ii in all_files:\n",
    "        if (\n",
    "            ii.startswith(var + \".\")\n",
    "            and model in ii\n",
    "            and variant in ii\n",
    "            and domain in ii\n",
    "        ):\n",
    "            if domain in ii:\n",
    "                read_files.append(os.path.join(modeldir, str(ii)))\n",
    "    assert len(read_files) > 0, f\"No matching files found in {modeldir}\"\n",
    "\n",
    "    del all_files\n",
    "\n",
    "    data = xr.open_mfdataset(read_files, combine=\"by_coords\")\n",
    "    var_read = data.variables[var]\n",
    "\n",
    "    dates = []\n",
    "    for val in data[\"day\"].data:\n",
    "        try:\n",
    "            dates.append(datetime.datetime.strptime(str(val)[0:-2], \"%Y%m%d\").date())\n",
    "        except ValueError:\n",
    "            dates.append(datetime.datetime(int(str(val)[0:4]), int(str(val)[4:6]), 28))\n",
    "\n",
    "\n",
    "    var_read = xr.DataArray(var_read, dims=[\"day\", \"lat2d\", \"lon2d\"])\n",
    "    var_read[\"day\"] = dates\n",
    "    return var_read\n",
    "\n",
    "## load data  \n",
    "var = 'snow'\n",
    "modeldir = '/glade/campaign/uwyo/wyom0112/postprocess/'\n",
    "model = 'ukesm1-0-ll'\n",
    "variant = 'r2i1p1f2'\n",
    "domain = 'd02'\n",
    "basedir = '/glade/u/home/mcowherd/'\n",
    "projectdir = basedir + 'fos-data/'\n",
    "snoteldir = projectdir + 'snoteldata/'\n",
    "wrfdir = '/glade/campaign/uwyo/wyom0112/postprocess/'\n",
    "wrfcoorddir = projectdir \n",
    "domain = \"d02\"\n",
    "\n",
    "## LOAD DATA ## \n",
    "mod_historical = model +'_'+ variant + '_historical_bc'\n",
    "mod_future = model +'_' + variant+ '_ssp370_bc'\n",
    "gcm = mod_historical\n",
    "date_start_pd, date_end_pd = [1980, 1, 1], [2013, 12, 31]  # 30 years, historical\n",
    "exp = \"hist\"\n",
    "var_wrf = wrfread(modeldir, gcm, exp, variant, domain, var)\n",
    "var_wrf = util.screen_times_wrf(var_wrf, date_start_pd, date_end_pd)\n",
    "\n",
    "# future dates\n",
    "date_start_pd, date_end_pd = [2014, 1, 1], [2100, 12, 31]\n",
    "gcm = mod_future\n",
    "model = \"ssp370\"\n",
    "var_wrf_ssp370 = wrfread(modeldir, gcm, model, variant, domain, var)\n",
    "var_wrf_ssp370 = util.screen_times_wrf(var_wrf_ssp370, date_start_pd, date_end_pd)\n",
    "\n",
    "wrfdata = [var_wrf, var_wrf_ssp370]\n",
    "\n",
    "coords = util.get_coords(wrfcoorddir+'/wrf_coordinates')\n",
    "\n",
    "lon_wrf = coords['lon2d']\n",
    "lat_wrf = coords['lat2d']\n",
    "\n",
    "\n",
    "\n",
    "wrf_snow_data = []\n",
    "for i, entry in snotelmeta[100:102].iterrows():\n",
    "    pt = [entry.lon, entry.lat]\n",
    "    dis = ((pt[0]-lon_wrf[:].data)**2+(pt[1]-lat_wrf[:].data)**2)**0.5\n",
    "    ind = np.unravel_index(dis.argmin(), dis.shape)\n",
    "    j = int(ind[0])\n",
    "    k = int(ind[1])\n",
    "    wrfts = get_wrf_data_points(wrfdata, j, k)\n",
    "    wrf_snow_data.append(wrfts)\n",
    "    \n",
    "N = len(wrf_snow_data[0])\n",
    "index = pd.date_range('1980-01-01', periods=N, freq='D')\n",
    "data = {'SNOTEL_SWE':wrf_snow_data[0]}\n",
    "forcing_data = pd.DataFrame(data, index=index)\n",
    "true_SWE = pd.DataFrame({'SWE': wrf_snow_data[1]}, index=index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fos.models import snotel_value, snotel_with_offset, training_mean, lin_reg\n",
    "from fos.util import partition_dataframe, make_time_lists\n",
    "mods_and_params = [(snotel_value, {'vars': ['SWE']}), \n",
    "                   (snotel_with_offset,  {'vars': ['SWE']}),\n",
    "                   (training_mean,  {'vars': ['SWE']}),\n",
    "                   (lin_reg,  {'vars': ['SWE'], 'fvars': ['SNOTEL_SWE']}),]\n",
    "\n",
    "# Define start and stop dates for three time periods\n",
    "time_periods = {'train': ('1980-01-01','2000-01-01'),\n",
    "           'validation': ('2000-01-01', '2020-01-01'),\n",
    "           'test': ('2020-01-01','2090-01-01')}\n",
    "\n",
    "\n",
    "forcing_split = partition_dataframe(forcing_data, time_periods)\n",
    "obs_split = partition_dataframe(true_SWE, time_periods)\n",
    "dates_lists = make_time_lists(time_periods)\n",
    "\n",
    "\n",
    "num = 123456789 ## get it from the thing\n",
    "datadir = '/glade/u/home/mcowherd/nh-WRF/'\n",
    "\n",
    "\n",
    "forcing_head = ['Year','Month','Day','Hr', 'SNOTEL_SWE']\n",
    "## usgs txt fmt = 123456789 1980 01 01 VALUE ## \n",
    "forcing_dir = datadir + '/basin_mean_forcing/nldas/'\n",
    "sf_dir = datadir + 'usgs_streamflow/'\n",
    "forcing_of = forcing_dir + f'{num}_forcing.txt'\n",
    "sf_of = sf_dir + f'{num}_streamflow.txt'\n",
    "nc_of = datadir + f'time_series/{num}.nc'\n",
    "\n",
    "df = forcing_data.copy()\n",
    "# Save DataFrame to tab-delimited text file with year, month, date, and value in separate columns\n",
    "df.reset_index(inplace=True)  # Reset index to use date as a column\n",
    "df.rename(columns={'index': 'date'}, inplace=True)  # Rename index column to date\n",
    "df['Year'] = df['date'].dt.year  # Extract year from date\n",
    "df['Mnth'] = df['date'].dt.month  # Extract month from date\n",
    "df['Day'] = df['date'].dt.day  # Extract day from date\n",
    "df = df[['Year', 'Mnth', 'Day', 'SNOTEL_SWE']]  # Reorder columns\n",
    "df.to_csv(forcing_of, sep='\\t', index=False)  # Save DataFrame to tab-delimited text file\n",
    "\n",
    "df = true_SWE.copy()\n",
    "df.reset_index(inplace=True)  # Reset index to use date as a column\n",
    "df.rename(columns={'index': 'date'}, inplace=True)  # Rename index column to da\n",
    "basin = [num for i in range(len(df))]\n",
    "df['basin'] = basin\n",
    "df['Year'] = df['date'].dt.year  # Extract year from date\n",
    "df['Mnth'] = df['date'].dt.month  # Extract month from date\n",
    "df['Day'] = df['date'].dt.day  # Extract day from date\n",
    "df = df[['basin', 'Year', 'Mnth', 'Day', 'SWE']]  # Reorder columns\n",
    "df.to_csv(sf_of, sep='\\t',header = False, index=False)  # Save DataFrame to tab-delimited text file\n",
    "\n",
    "\n",
    "\n",
    "# Read in tab-delimited text file and create DataFrame\n",
    "df = pd.read_csv(forcing_of, sep='\\t', header=0, names=['year', 'month', 'day', 'SWE'])\n",
    "df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "df = df.set_index('date')\n",
    "df = df.drop(['year', 'month', 'day'], axis=1)\n",
    "\n",
    "true_df = pd.read_csv(sf_of, sep='\\t', header=0, names=['year', 'month', 'day', 'SNOTEL_SWE'])\n",
    "true_df['date'] = pd.to_datetime(true_df[['year', 'month', 'day']])\n",
    "true_df = true_df.set_index('date')\n",
    "true_df = true_df.drop(['year', 'month', 'day'], axis=1)\n",
    "\n",
    "df = pd.merge(df, true_df, on=['date'])\n",
    "# Convert DataFrame to xarray Dataset and create 'date' coordinate from year, month, and day columns\n",
    "ds = xr.Dataset.from_dataframe(df)\n",
    "\n",
    "# Save xarray Dataset to netCDF file\n",
    "ds.to_netcdf(nc_of)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prep data for LSTM:\n",
    "    all points, just peak data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpath = \"data/ukesm\"\n",
    "plotpath = \"plots/dev\"\n",
    "\n",
    "from fos.util import setup_plot_style\n",
    "from fos.util import memory\n",
    "\n",
    "os.makedirs(plotpath, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HELPERS\n",
    "def read_csvs(dpath: str) -> pd.DataFrame:\n",
    "  dfs = list()\n",
    "  for filename in dpath:\n",
    "      data = pd.read_csv(filename, index_col=None, header=0)\n",
    "      data['pt'] = filename.stem.split('_')[-1]\n",
    "      dfs.append(data)\n",
    "  df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "  df = df.rename(columns={'Unnamed: 0': 'wyear'})\n",
    "  return df\n",
    "\n",
    "\n",
    "@memory.cache\n",
    "def get_data(dpath: str) -> pd.DataFrame:  \n",
    "  basins = read_csvs(Path(dpath).glob(\"wrfb*.csv\"))\n",
    "  pts = read_csvs(Path(dpath).glob(\"wrfp*.csv\"))\n",
    "  combined_df = pd.merge(pts, basins, how='left', left_on=['pt', 'wyear'], right_on=['pt', 'wyear'], suffixes=('_pt', '_basin'))\n",
    "  return combined_df\n",
    "\n",
    "\n",
    "def savefig(name: str):\n",
    "  plt.savefig(os.path.join(plotpath, name))\n",
    "\n",
    "\n",
    "def plt_err_hist(dffield, xlabel, ylabel, title, fname):\n",
    "  plt.clf()\n",
    "  sns.histplot(dffield)\n",
    "  plt.xlabel(xlabel)\n",
    "  plt.ylabel(ylabel)\n",
    "  plt.title(title)\n",
    "  savefig(fname)\n",
    "  plt.show()\n",
    "\n",
    "def plt_year_err(data, title, xlab, ylab, fname):\n",
    "  plt.clf()\n",
    "  mean_data = data.mean()\n",
    "  std_data = data.std()\n",
    "  mean_data.plot()\n",
    "  plt.fill_between(mean_data.index, mean_data - std_data, mean_data +  std_data, color=\"b\", alpha=0.2);\n",
    "  plt.ylabel(ylab)\n",
    "  plt.xlabel(xlab)\n",
    "  plt.title(title)\n",
    "  savefig(fname)\n",
    "  plt.show()\n",
    "  \n",
    "def plot_relation(df, x, y, title, fname, c='wyear', colormap='viridis'):\n",
    "  plt.clf()\n",
    "  df.plot.scatter(x=x, y=y, c=c, colormap=colormap, title=title)\n",
    "  plt.gca().axline((0, 0), slope=1)\n",
    "  savefig(fname)\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped 5616/96408 rows with NaNs\n"
     ]
    }
   ],
   "source": [
    "# read in the data from dpath\n",
    "df = get_data(dpath)\n",
    "df.head()\n",
    "\n",
    "orig_len = len(df)\n",
    "# clean the data\n",
    "df.dropna(inplace=True)\n",
    "new_len = len(df)\n",
    "print(f\"dropped {orig_len - new_len}/{orig_len} rows with NaNs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fos.data import huc6, in6s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## goal: save these as nc files\n",
    "## one nc file per basin with name nc_of = datadir + f'time_series/{num}.nc'\n",
    "## date index\n",
    "## in each file, all the point values for the poi# and possibly all the other derived variables once we get to that point? \n",
    "## then can try different version of each -- combinations of all of the different variables\n",
    "## utkarsh for predictors\n",
    "## shiheng for methods\n",
    "## earlier stuff (multivariable regression) for argument that it will decay\n",
    "basin_ids = np.unique(in6s) ## 91 total ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from fos.dirs import basedir, projectdir, snoteldir\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "'''\n",
    "snotelmeta\n",
    "snotel_gdf\n",
    "in6s, in8s\n",
    "snotel_no_ak\n",
    "'''\n",
    "\n",
    "snotelmeta = pd.read_csv(snoteldir + 'snotelmeta.csv')\n",
    "huc6 = gpd.read_file(projectdir + 'spatialdata/huc6.shp')\n",
    "huc8 = gpd.read_file(projectdir + 'spatialdata/huc8.shp')\n",
    "snotel_gdf = gpd.GeoDataFrame(data = {'site_name':snotelmeta.site_name,\n",
    "                                     'elev': snotelmeta.elev,\n",
    "                                     'site_number':snotelmeta.site_number,\n",
    "                                     'state':snotelmeta.state,\n",
    "                                     'namestr':snotelmeta.namestr,\n",
    "                                     'startdt':snotelmeta.startdt}, geometry = gpd.points_from_xy(snotelmeta.lon, snotelmeta.lat), crs = 'epsg:4326')\n",
    "\n",
    "## time series of the difference in time between peak SWE at the grid box where the SNOTEL resides \n",
    "## and the time of peak SWE across the HUC-6 watershed in which the SNOTEL resides\n",
    "in6 = []\n",
    "in8 = []\n",
    "ptnames = []\n",
    "for i in snotel_gdf.index:\n",
    "    point = snotel_gdf[snotel_gdf.index == i].geometry[i]\n",
    "    ptname = snotel_gdf[snotel_gdf.index == i].site_number[i]\n",
    "    basin = huc6[huc6.contains(point)]\n",
    "    k = basin.name.index[0]\n",
    "    in6.append(basin.name[k])\n",
    "    basin = huc8[huc8.contains(point)]\n",
    "    k = basin.name.index[0]\n",
    "    in8.append(basin.name[k])\n",
    "    ptnames.append(ptname)\n",
    "in6s = pd.Series(in6, index = ptnames)\n",
    "in8s = pd.Series(in8, index = ptnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in6s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fos",
   "language": "python",
   "name": "fos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "ab45ce4c837ec36a23cd65e47f03d45270f377f095737e0108128022c1171ca7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
